{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c17e1b4b",
   "metadata": {},
   "source": [
    "<h1 style = \"text-align: center;color: #206bf7\">Developing a data pipeline APIs to Cloud with Python</h1>\n",
    "<p></p>\n",
    "<center> <p style = \"text-align: center\" >Joachim Schulze</p> </center>\n",
    "<center> <p style = \"text-align: center\" >June 2022</p> </center>\n",
    "<p></p>\n",
    "<p></p>\n",
    "<p></p>\n",
    "<p></p>\n",
    "<p></p>\n",
    "<center><img src = \"https://cdn.pixabay.com/photo/2018/11/28/10/45/cloud-3843352_960_720.jpg\" style=\"width:700px;height:350px;\"></img></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0781c57d",
   "metadata": {},
   "source": [
    "<center><h2 style=\"color: #05a5f5\">The Project's Objectives and  Environment</h2></center>\n",
    "<p style=\"text-align: justify\">The objective of the project was to develope my skills in creating a phyton based datapipeline. Various APIs were to be used to \n",
    "to retrieve data and store them in a structured way (database) in a cloud. The data within the database should be updated if appropriate.\n",
    "Based on these structured and up to date informations I should be able to make profound and valuable recommendations to a company called \"Gans\". The company does rent scooters\n",
    "in various European Cities. To this end I virtually became a Data Scientist at Gans with the task to obtain publically available information that could help\n",
    "the company to strategically and \"data-based\" distribute their scooters in the cities, make intelligent assumptions were to expect large numbers of left scooters and\n",
    "where and when a high scooter-demand will occurr. The buisness of scooter rental is a comparatively young service that is in large parts digitized. Both facts, the \n",
    "relatively little extent of experience in the dynamic timely and spacially distribution of the scooter-demand and the high degree of digitalization of the service make it an ideal\n",
    "place for data-based decision development in the service. The directs steps in which these objectives were suppossed to be acchieved were:\n",
    "<ul style=\"margin: 20px\"><li style=\"margin: 20px\"><span style=\"color: #d13a11\"><strong>1. Data Retrieval and Collection:</strong></span> Write phyton request to usefull API's and store the data in clearly structured dataframes locally.</li>\n",
    "<li style=\"margin: 20px\"><span style=\"color: #d13a11\"><strong>2. Data Transfer to a Cloud:</strong></span> Transfer the data to a professional cloud service. Use a serverless offer(see below).</li>\n",
    "<li style=\"margin: 20px\"><span style=\"color: #d13a11\"><strong>3. Data Updating in the Cloud:</strong></span> Decide and implement which part of the database should be fed with repeated updates.</li></ul></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d635f27",
   "metadata": {},
   "source": [
    "<h3 style=\"color: #d13a11\">1. Data Retrival and Collection</h3>\n",
    "<p style=\"text-align: justify\">For the first objective I selected four APIs for data retrival. The first one was <a href=\"https://geodb.com/\">GeoDB</a> for obtaining basic data on the\n",
    "cities were the scooters are rented. For example the population if the city will be important for decisions about the extent of the service provided by Gans.\n",
    "The GeoDB API demands a \"wikidata id\" of the city which needed to be obtained by web-scrapping using the beautifull soup package of python. The following code was used to obtain the wikidata-id.\n",
    "\\d+ is a regular expression and means one digit or more, the wiki data id consist of a Q followed by severaldigits</p>"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8c912480",
   "metadata": {},
   "source": [
    "Code-Snippet 1:\n",
    "_______________________________________________________________\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import re\n",
    "\n",
    "a = ['Berlin', 'Paris']\n",
    "\n",
    "def idScrapping(cities):\n",
    "    cities_id = [] # initiate an empty id list\n",
    "    for city in cities:\n",
    "        #retrieve the wikidataId\n",
    "        time.sleep(1)\n",
    "        url1 = 'https://en.wikipedia.org/wiki/{}'.format(city) \n",
    "        citem = requests.get(url1, 'html.parser') \n",
    "        if BeautifulSoup(citem.content) != None:\n",
    "            soup = BeautifulSoup(citem.content)\n",
    "        if soup.find('li', {'id':'t-wikibase'}).find('a')['href'] != None:\n",
    "            wikidata_link = soup.find('li', {'id':'t-wikibase'}).find('a')['href'] \n",
    "        city_id = re.search('Q\\d+', wikidata_link).group()\n",
    "        cities_id.append(city_id)\n",
    "      \n",
    "    return cities_id\n",
    "\n",
    "idScrapping(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e8bc2a1",
   "metadata": {},
   "source": [
    "<p style=\"text-align: justify\">The 'beautifull soup' python library simplifies the finding of information in a website`s html- or xml-code. The wiki-data-id consist of a 'Q' followed by several digits. Here I used a regular expression to retrieve this were:\n",
    "'\\d+' means one digit or more. The return is a list of the cities wiki-data-id. Using this list the geoDB API can be used to retrieve data about the cities. A simplified way how I did this is shown here:  </p>"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8a5cf492",
   "metadata": {},
   "source": [
    "Code-Snippet 2:\n",
    "_________________________________________________________________________________\n",
    "import requests\n",
    "import pandas as pd\n",
    "import json\n",
    "import mysql.connector\n",
    "\n",
    "aa = ['Berlin', 'Paris']\n",
    "\n",
    "def cities_info(cities):\n",
    "    dfList = []\n",
    "    for city in cities:\n",
    "        url2 = \"https://wft-geo-db.p.rapidapi.com/v1/geo/cities/{}\".format(city_id)\n",
    "        headers = {\n",
    "        \"X-RapidAPI-Key\": API_key,\n",
    "        \"X-RapidAPI-Host\": \"wft-geo-db.p.rapidapi.com\"\n",
    "        }    \n",
    "        response = requests.request(\"GET\", url2, headers=headers)\n",
    "        print(response.json())\n",
    "        cit_dic = {}#make a dictionary to retrieve the information\n",
    "        cit_dic['City'] = response.json()['data']['name']\n",
    "        cit_dic['Country'] = response.json()['data']['country']\n",
    "        cit_dic['CountryCode'] = response.json()['data']['countryCode']\n",
    "        cit_dic['WikiDataId'] = response.json()['data']['wikiDataId']\n",
    "        cit_dic['Latitude'] = round(response.json()['data']['latitude'], 4)\n",
    "        cit_dic['Longitude'] = round(response.json()['data']['longitude'], 4)\n",
    "        cit_dic['Population'] = response.json()['data']['population']\n",
    "        cit_dic['Timezone'] = response.json()['data']['timezone']\n",
    "        \n",
    "        dfList.append(cit_dic) #put it in a list\n",
    "        \n",
    "    df_demo = pd.DataFrame(dfList) # transform the list to df\n",
    "    df_demo.to_csv('./csvTables/City.csv') #writes the df to csv \n",
    "    return df_demo\n",
    "\n",
    "cities_info(aa) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4da4032f",
   "metadata": {},
   "source": [
    "<p style=\"text-align: justify\">The code needs the list 'city_id'. The items from the list are appended to the url by the \n",
    "function 'format()'. The two steps of the data collection are here shown in separate functions but \n",
    "should be used in one were they can be combined in one for-loop. \n",
    "For a beginning the dataframes are saved locally as csv file. The next step is to send the dataframes in a table a local sql-database.\n",
    "For the project I produced for dataframes following the same principle workflow - with or without the help of 'soup'. I collected in addition to the information \n",
    "on the cities, weather-data, and eventually data on the cities airport with data about incomming and outgoing flights. All in all four dataframes.\n",
    "These dataframes were send to a sql-database locally which was set up by the use of MySQL. The schema was called 'scooter'. The connection was insured with the following code were the last four lines send the \n",
    "dataframe cities to the database.</p>"
   ]
  },
  {
   "cell_type": "raw",
   "id": "28a91754",
   "metadata": {},
   "source": [
    "Code-Snippet 3:\n",
    "__________________________________________________________________________\n",
    "# define connection details\n",
    "cnx = mysql.connector.connect(\n",
    "    user='root',\n",
    "    password=<password>, #type your root password here\n",
    "    host='127.0.0.1', # to connect to your local instance\n",
    "    database='scooter' #type the name of the database you want to use here\n",
    ")\n",
    " \n",
    "# connect to database\n",
    "cursor   = cnx.cursor()\n",
    "schema   =\"scooter\"\n",
    "host     =\"127.0.0.1\"\n",
    "user     =\"root\"\n",
    "password = <password>\n",
    "port     = 3306\n",
    "con      = f'mysql+pymysql://{user}:{password}@{host}:{port}/{schema}'\n",
    "\n",
    "# send the dataframe to the local database\n",
    "cities_info(aa).to_sql('cities', \n",
    "                if_exists='append', \n",
    "                con=con, \n",
    "                index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77227d7d",
   "metadata": {},
   "source": [
    "<p style=\"text-align: justify\">Finally the defining of primary and foreign keys within the database tables were also made by using the MySQL Gui. \n",
    "The use of these keys allows the efficient work with the database using the sql-language. The structure of the database is shown in the next picture. </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "793aa1c0",
   "metadata": {},
   "source": [
    "![](databaseStructure.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b1203b1",
   "metadata": {},
   "source": [
    "<h3 style=\"color: #d13a11\">2. Data Transfer to a Cloud:</h3>\n",
    "<p style=\"text-align: justify\">After collecting and locally storing the data, the next step in the project was to transfer the data\n",
    "to a cloud. I decided to use a serverless cloud architecture. This means bassically that you do not need to decide on which hardware and software, including potentially necessary middleware, \n",
    "you use on the serverside.A further advantage is the scalability of the service, so that you basically only pay for what you really need and use.\n",
    "I choose the AWS-service that allows to implement backend functionality by so-called \"Lambda-function\" (not to confuse with the structure of Lambda function\n",
    "in several programming languages, including python). For the current project the free'basic-package' of the AWS-service was sufficient.\n",
    "Signing up for the service to receive endpoint access is mandatorily accompanied by providing credit card credentials. But basic service is at \n",
    "the moment free for the period of one year and, as mentioned, scalable. \n",
    "Using MySQL I configured a database coressponding to the local one on the AWS cloud. Sending the dataframes to the cloud could be done by basically only slightly changing \n",
    "code snippet 3, bassically providing the endpoint of your aes database and the credentials accordingly. \n",
    "</p>"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1cac5215",
   "metadata": {},
   "source": [
    "Code-Snippet 4:\n",
    "________________________________________________________________________________\n",
    "cnx = mysql.connector.connect(\n",
    "    user='admin',\n",
    "    password=<password, \n",
    "    host=<endpoint of aws bucket>, \n",
    "    database=<database name> \n",
    ")\n",
    " \n",
    "# connect to database\n",
    "cursor = cnx.cursor()\n",
    "schema=\"gans\"\n",
    "host=<endpoint of aws bucket>,\n",
    "user=\"admin\"\n",
    "password=<endpoint of aws bucket>,\n",
    "port=3306\n",
    "con = f'mysql+pymysql://{user}:{password}@{host}:{port}/{schema}'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa6d481",
   "metadata": {},
   "source": [
    "<h3 style=\"color: #d13a11\">3. Data Updating in the Cloud</h3>\n",
    "<p style=\"text-align: justify\">The AWS service provides the possibility to obtain and update the tables in the cloud-database\n",
    "using so called \"Lambda functions\". The function do need only a few adaptations and can be run from a \"handler\" provided as the core\n",
    "of each Lambda function. The enviroment-setting for these functions is provided by so-called layers, for which the implementation requires carefull\n",
    "reading of the documentation. The slightly simplified structure of the weather function is shown in code-snippet 5 for two airports: Once successfully implemented the database can not only filled but automatically updated from the cloud.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "raw",
   "id": "58d4b29d",
   "metadata": {},
   "source": [
    "Code_Snippet 5:\n",
    "____________________________________________________________________________________________________________________\n",
    "API_KE = 'key' #put your key here\n",
    "cit = ['EDSB', 'EDDT']\n",
    "def flights(cities):\n",
    "    \n",
    "    a = {'City': [], 'FlightNumber': [], 'FlightStatus': [], 'OriginAirport': [], 'ScheduledTime': [], 'AirLine': []}\n",
    "    for city in cities:\n",
    "        url = f\"https://aerodatabox.p.rapidapi.com/flights/airports/icao/{city}/2022-06-15T06:00/2022-06-15T17:00\"\n",
    "        querystring = \n",
    "        {\"direction\":\"Arrival\",\"withCodeshared\":\"true\",\"withCargo\":\"false\",\"withPrivate\":\"false\",\"withLocation\":\"true\"}\n",
    "        headers = {\n",
    "           \"X-RapidAPI-Host\": \"aerodatabox.p.rapidapi.com\",\n",
    "           \"X-RapidAPI-Key\": API_KE\n",
    "           }\n",
    "        responses = requests.request(\"GET\", url, headers=headers, params=querystring)\n",
    "        json = responses.json()\n",
    "        \n",
    "        for i in json['arrivals']:\n",
    "            a['City'] = city\n",
    "            a['FlightNumber'].append(i['number'])\n",
    "            a['FlightStatus'].append(i['status'])\n",
    "            a['OriginAirport'].append(i['movement']['airport']['name'])\n",
    "            a['ScheduledTime'].append(i['movement']['scheduledTimeLocal'])\n",
    "            a['AirLine'].append(i['airline']['name'])\n",
    "        \n",
    "    c = pd.DataFrame(a)\n",
    "    \n",
    "    schema=\"scooter\"\n",
    "    host=<ep> # give the according endpoint here\n",
    "    user=\"admin\"\n",
    "    password=<pw> #provide your password\n",
    "    port=3306\n",
    "    #con = f'mysql+pymysql://{user}:{password}@{host}:{port}/{schema}'\n",
    "    c.to_sql('flight',\n",
    "    f'mysql+pymysql://{user}:{password}@{host}:{port}/{schema}',\n",
    "    if_exists='append',\n",
    "    index=True)\n",
    "  \n",
    "\n",
    "\n",
    "def lambda_handler(event, context):\n",
    "    flights(cit)\n",
    "    return {\n",
    "        'statusCode': 200,\n",
    "        'body': json.dumps('Hello, your flight data are saved')\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5158d15",
   "metadata": {},
   "source": [
    "<p style=\"text-align: justify\">Once successfully implemented the database can not only filled but also be automatically updated from the cloud. Since from my\n",
    "four tables two were more or less static = cities and airports, I implemented Lambda functions only for weather and flights. To update \n",
    "these tables on a scheduled basis you can use the \"Event-Bridge Function\". Using the AWS websites you have to go from the console => services =>\n",
    "Amazone Event Bridges => Rules. Here click create rules and follow the instructions using \"cron\" <a href=\"https://www.programmertools.online/generator/cron_expression.html\">how to make a cron</a> to define the event intervals.\n",
    "The Event-bridge is now linked to your function an can for example daily update your data automatically.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa58a4e",
   "metadata": {},
   "source": [
    "<center><h2 style=\"color: #05a5f5\">Conclusions</h2></center>\n",
    "<p>The pipeline delivers useful data to predict scooter demand based on weather and visitor numbers of the cities. Here a combination\n",
    "with the companies geographical data would be very beneficial as for example the occurrence of clusters in high scooter demand or leaving \n",
    "could be related to occuring weather events or flights. An extension of the visitor estimates could be achieved by including the railways stations\n",
    "and the according traveling data as well as traffic data on incomming highways. The database should be connected to local geographical databases.\n",
    "Overall can be said that efforts in data use for increasing the efficiency of scooter rental appear to be a very promising approach that will \n",
    "increase the revenue of the service!</p>\n",
    "<p>Find more on the project at: <a href=\"https://github.com/Joachim0211/DatapipelineApiToCloud\">scooter on githup</a></p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
