{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f393b88",
   "metadata": {},
   "source": [
    "<center> <h1>Developing a data pipeline APIs to Cloud with Python</h1> </center>\n",
    "<center><img src = \"https://cdn.pixabay.com/photo/2018/11/28/10/45/cloud-3843352_960_720.jpg\" style=\"width:700px;height:400px;\"></img></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c87e80c",
   "metadata": {},
   "source": [
    "<center><h2>The Project's Objectives and  Environment</h2></center>\n",
    "<p style=\"text-align: justify\">The objective of the project was to develope my skills in creating a phyton based datapipeline. Various APIs were to be used to \n",
    "to retrieve data and store them in a structured way (database) in a cloud. The data within the database should be updated if appropriate.\n",
    "Based on these structured and up to date informations I should be able to make profound and valuable recommendations to a company called \"Gans\". The company does rent scooters\n",
    "in various European Cities. To this end I virtually became a Data Scientist at Gans with the task to obtain publically available information that could help\n",
    "the company to strategically and \"data-based\" distribute their scooters in the cities, make intelligent assumptions were to expect large numbers of left scooters and\n",
    "where and when a high scooter-demand will occurr. The buisness of scooter rental is a comparatively young service that is in large parts digitized. Both facts, the \n",
    "relatively little extent of experience in the dynamic timely and spacially distribution of the scooter-demand and the high degree of digitalization of the service make it an ideal\n",
    "place for data-based decision development in the service. The directs steps in which these objectives were suppossed to be acchieved were:\n",
    "<ul style=\"margin: 20px\"><li style=\"margin: 20px\">1. <strong>Data Retrieval and Collection:</strong> Write phyton request to usefull API's and store the data in clearly structured dataframes locally.</li>\n",
    "<li style=\"margin: 20px\"><span style=\"color: #d13a11\"><strong>2. Data Transfer to a Cloud:</strong></span> Transfer the data to a professional cloud service. Use a serverless offer(see below).</li>\n",
    "<li style=\"margin: 20px\">3. <strong>Data Updating in the Cloud:</strong> Decide and implement which part of the database should be fed with repeated updates.</li></ul></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c086473",
   "metadata": {},
   "source": [
    "<h3>Data Retrival and Collection</h3>\n",
    "<p style=\"text-align: justify\">For the first objective I selected four APIs for data retrival. 1. GeoDB for obtaining basic data on the\n",
    "cities were the scooters are rented. For example the population if the city will be important for decisions about the extent of the service provided by Gans.\n",
    "The GeoDB API demands a \"wikidata id\" of the city which needed to be obtained by web-scrapping using the beautifull soup package of python. The following code was used to obtain the wikidata-id.\n",
    "\\d+ is a regular expression and means one digit or more, the wiki data id consist of a Q followed by severaldigits</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e54c6c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import re\n",
    "\n",
    "a = ['Berlin', 'Paris']\n",
    "\n",
    "def idScrapping(cities):\n",
    "    cities_id = [] # initiate an empty id list\n",
    "    for city in cities:\n",
    "        #retrieve the wikidataId\n",
    "        time.sleep(1)\n",
    "        url1 = 'https://en.wikipedia.org/wiki/{}'.format(city) \n",
    "        citem = requests.get(url1, 'html.parser') \n",
    "        if BeautifulSoup(citem.content) != None:\n",
    "            soup = BeautifulSoup(citem.content)\n",
    "        if soup.find('li', {'id':'t-wikibase'}).find('a')['href'] != None:\n",
    "            wikidata_link = soup.find('li', {'id':'t-wikibase'}).find('a')['href'] \n",
    "        city_id = re.search('Q\\d+', wikidata_link).group()\n",
    "        cities_id.append(city_id)\n",
    "      \n",
    "    return cities_id\n",
    "\n",
    "idScrapping(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28684fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "<p style=\"text-align: justify\">The 'beautifull soup' python library simplifies the finding of information in a website`s html- or xml-code. The wiki-data-id consist of a 'Q' followed by several digits. Here I used a regular expression to retrieve this were:\n",
    "'\\d+' means one digit or more. The return is a list of the cities wiki-data-id. Using this list the geoDB API can be used to retrieve data about the cities. A simplified way how I did this is shown here:  </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59fae0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cities_info(cities):\n",
    "    dfList = []\n",
    "    for city in cities:\n",
    "        url2 = \"https://wft-geo-db.p.rapidapi.com/v1/geo/cities/{}\".format(city_id)\n",
    "        headers = {\n",
    "        \"X-RapidAPI-Key\": API_key,\n",
    "        \"X-RapidAPI-Host\": \"wft-geo-db.p.rapidapi.com\"\n",
    "        }    \n",
    "        response = requests.request(\"GET\", url2, headers=headers)\n",
    "        print(response.json())\n",
    "        cit_dic = {}#make a dictionary to retrieve the information\n",
    "        cit_dic['City'] = response.json()['data']['name']\n",
    "        cit_dic['Country'] = response.json()['data']['country']\n",
    "        cit_dic['CountryCode'] = response.json()['data']['countryCode']\n",
    "        cit_dic['WikiDataId'] = response.json()['data']['wikiDataId']\n",
    "        cit_dic['Latitude'] = round(response.json()['data']['latitude'], 4)\n",
    "        cit_dic['Longitude'] = round(response.json()['data']['longitude'], 4)\n",
    "        cit_dic['Population'] = response.json()['data']['population']\n",
    "        cit_dic['Timezone'] = response.json()['data']['timezone']\n",
    "        \n",
    "        dfList.append(cit_dic) #put it in a list\n",
    "        \n",
    "    df_demo = pd.DataFrame(dfList) # transform the list to df\n",
    "    df_demo.to_csv('./csvTables/City.csv') #writes the df to csv to the folder csv tables and replaces the table of the same name that might be already there\n",
    "    return df_demo\n",
    "\n",
    "demo(citd) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a1d7da",
   "metadata": {},
   "outputs": [],
   "source": [
    "<p style=\"text-align: justify\">The code needs the list 'city_id'. The items from the list are appended to the url by the function 'format()'. The two steps of the data collection are here shown in separate functions but should be used in one were they can be compined in one for-loop. For a beginning the dataframes are saved locally as csv file. </p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
